# QIW Metrics and Reporting

**Document ID**: QIW-METRICS-001  
**Version**: 1.0.0  
**Date**: 2026-01-14  
**Authority**: PartPulse QA Team + Governance Liaison

---

## Purpose

This document defines the metrics tracked by QIW, reporting cadence, KPIs, and analysis guidelines for quality monitoring and governance compliance.

---

## Metric Categories

### 1. Quality Indicators
Measure overall quality and incident trends

### 2. Process Indicators
Measure effectiveness of QIW process itself

### 3. Channel-Specific Metrics
Measure health of individual observation channels

---

## Quality Indicators

### Overall Quality Score

**Definition**: Composite score (0-100) across all 5 channels  
**Formula**:
```
Quality Score = (
  (Build Score * 0.20) +
  (Lint Score * 0.15) +
  (Test Score * 0.30) +
  (Deployment Score * 0.15) +
  (Runtime Score * 0.20)
)
```

**Targets**:
- Excellent: 90-100
- Good: 75-89
- Acceptable: 60-74
- Needs Improvement: <60

**Tracking**: Continuous, reported daily

---

### Incident Count

**Definition**: Total number of incidents by severity over time period

**Metrics**:
- Total incidents
- Critical incidents
- High severity incidents
- Medium severity incidents
- Low severity incidents

**Targets**:
- Critical: 0 per week
- High: <5 per week
- Medium: <15 per week
- Low: <30 per week

**Tracking**: Daily/Weekly/Monthly

**Analysis**:
- Trend over time (increasing/decreasing)
- Distribution by channel
- Repeat incidents (same root cause)

---

### Mean Time To Detection (MTTD)

**Definition**: Average time from anomaly occurrence to QIW detection

**Formula**:
```
MTTD = Œ£(Detection Time - Anomaly Start Time) / Number of Incidents
```

**Targets**:
- Build: <5 minutes
- Lint: <5 minutes (pre-commit)
- Test: <5 minutes
- Deployment: <10 minutes
- Runtime: <5 minutes

**Tracking**: Monthly

---

### Mean Time To Remediation (MTTR)

**Definition**: Average time from detection to resolution

**Formula**:
```
MTTR = Œ£(Resolution Time - Detection Time) / Number of Resolved Incidents
```

**Targets by Severity**:
- Critical: <4 hours
- High: <24 hours
- Medium: <7 days
- Low: <30 days

**Tracking**: Daily/Weekly/Monthly

**Breakdown**:
- MTTR by severity
- MTTR by channel
- MTTR by team
- MTTR trend over time

---

### Repeat Incident Rate

**Definition**: Percentage of incidents with same root cause as previously resolved incident

**Formula**:
```
Repeat Rate = (Repeat Incidents / Total Incidents) * 100
```

**Target**: <10%

**Tracking**: Monthly

**Analysis**: Indicates effectiveness of root cause fixes

---

### SLA Compliance Rate

**Definition**: Percentage of incidents resolved within SLA

**Formula**:
```
SLA Compliance = (Incidents Resolved Within SLA / Total Resolved Incidents) * 100
```

**Target**: >95%

**Tracking**: Weekly/Monthly

**Breakdown**:
- By severity
- By channel
- By team

---

## Process Indicators

### False Positive Rate

**Definition**: Percentage of incidents marked as false positives

**Formula**:
```
False Positive Rate = (False Positives / Total Incidents) * 100
```

**Target**: <2%

**Tracking**: Weekly

**Analysis**:
- By detector
- By channel
- Root cause of false positives

**Action**: Adjust detector thresholds if FP rate exceeds 5%

---

### Gate Block Rate

**Definition**: Percentage of PRs/deployments blocked by quality gates

**Formula**:
```
Block Rate = (Blocked Items / Total Items) * 100
```

**Target**: <5% (indicates good quality practices)

**Tracking**: Weekly

**Analysis**:
- Block rate by gate (PR vs. Deployment)
- Block rate by channel
- Trend over time

**Note**: Very low (<1%) may indicate gates too lenient. Very high (>10%) may indicate systemic quality issues.

---

### Escalation Rate

**Definition**: Percentage of incidents requiring escalation

**Formula**:
```
Escalation Rate = (Escalated Incidents / Total Incidents) * 100
```

**Target**: <10%

**Tracking**: Monthly

**Breakdown**:
- By escalation level
- By incident severity
- By channel

---

### Detector Health

**Definition**: Reliability and effectiveness of automated detectors

**Metrics**:
- Detector uptime (% time operational)
- Detector accuracy (true positives / (true positives + false positives))
- Detector coverage (% of anomalies caught)

**Targets**:
- Uptime: >99%
- Accuracy: >95%
- Coverage: >90%

**Tracking**: Weekly

---

### Quality Gate Bypass Rate

**Definition**: Percentage of gates bypassed with approval

**Formula**:
```
Bypass Rate = (Bypassed Gates / Total Gates) * 100
```

**Target**: <1%

**Tracking**: Monthly

**Analysis**: Each bypass should be reviewed for patterns

---

## Channel-Specific Metrics

### Build Channel

**Metrics**:
- Build success rate
- Build duration (p50, p95)
- Dependency resolution failures
- Build failure rate on main branch

**Targets**:
- Success rate: >95%
- Duration increase: <10% vs baseline
- Failures on main: <5% per week

---

### Lint Channel

**Metrics**:
- Total lint violations
- Violations by severity (error, warning)
- Code complexity (cyclomatic complexity)
- Lint bypass count

**Targets**:
- Critical violations: 0 on main
- Total violations: Not increasing
- Complexity: <15 per function

---

### Test Channel

**Metrics**:
- Test pass rate
- Test coverage (line, branch, statement)
- Flaky test count
- Test dodging incidents
- Test debt (skipped tests)
- Test execution time

**Targets**:
- Pass rate: >95%
- Coverage: >80% (or baseline)
- Flaky tests: <5 total
- Test dodging: 0 per week
- Execution time: <10 minutes

---

### Deployment Channel

**Metrics**:
- Deployment success rate
- Deployment duration
- Rollback frequency
- Environment drift count
- Post-deployment health check failures

**Targets**:
- Success rate: >90%
- Duration: <30 minutes
- Rollbacks: <2 per month
- Environment drift: 0
- Health check failures: 0

---

### Runtime Channel

**Metrics**:
- Error rate (4xx, 5xx)
- Response time (p50, p95, p99)
- CPU utilization
- Memory utilization
- Exception count
- SLA compliance

**Targets**:
- Error rate: <1%
- P95 response time: <500ms
- CPU utilization: <70%
- Memory: Stable (no leaks)
- SLA compliance: >99%

---

## Reporting Cadence

### Real-Time

**Audience**: All engineers  
**Content**: Critical incidents  
**Delivery**: Dashboard, Slack/email alerts  

**Includes**:
- Critical incident alerts
- Quality gate block notifications
- SLA expiration warnings

---

### Daily

**Audience**: Team members  
**Content**: Daily incident summary  
**Delivery**: Email digest (08:00)  

**Includes**:
- Incidents detected yesterday
- Incidents resolved yesterday
- Active critical/high incidents
- PRs currently blocked
- MTTR for resolved incidents

**Template**:
```
QIW Daily Digest - 2026-01-14

Channel Health:
  Build: üü¢ 98  Lint: üü¢ 95  Test: üü° 87  Deploy: üü¢ 92  Runtime: üü¢ 96

Yesterday's Activity:
  ‚Ä¢ 3 new incidents detected (1 high, 2 medium)
  ‚Ä¢ 5 incidents resolved
  ‚Ä¢ MTTR: 6.2 hours (target: <8h)

Active Critical/High Incidents: 2
  ‚Ä¢ QIW-TEST-20260114-001 - Test pass rate at 92% (2h old, INVESTIGATING)
  ‚Ä¢ QIW-LINT-20260114-002 - Lint violations increased (5h old, INVESTIGATING)

Blocked PRs: 3
  ‚Ä¢ PR #456 - Blocked by QIW-TEST-20260114-001
  ‚Ä¢ PR #457 - Blocked by QIW-LINT-20260114-002
  ‚Ä¢ PR #458 - Blocked by QIW-BUILD-20260113-045

Actions Needed:
  ‚Ä¢ Fix failing tests in auth module
  ‚Ä¢ Address lint violations before merge to main
  ‚Ä¢ Resolve build configuration issue

View full dashboard: /governance/qiw/dashboard
```

---

### Weekly

**Audience**: Team + Team Leads  
**Content**: Weekly trends and analysis  
**Delivery**: Email (Monday 09:00)  

**Includes**:
- Quality score trend
- Incident count by channel and severity
- MTTR metrics
- SLA compliance
- Top issues
- Recommendations

**Template**:
```
QIW Weekly Digest - Week of 2026-01-07

Overall Quality Score: 93 (‚Üë +2 from last week) üü¢

Incidents This Week:
  ‚Ä¢ Total: 24 (‚Üì -3 from last week)
  ‚Ä¢ Critical: 0 ‚úì
  ‚Ä¢ High: 4 (‚Üë +1)
  ‚Ä¢ Medium: 12 (‚Üì -3)
  ‚Ä¢ Low: 8 (‚Üì -1)

MTTR by Severity:
  ‚Ä¢ Critical: N/A (no incidents)
  ‚Ä¢ High: 18.5h (target: <24h) ‚úì
  ‚Ä¢ Medium: 4.2d (target: <7d) ‚úì
  ‚Ä¢ Low: 12.5d (target: <30d) ‚úì

SLA Compliance: 87% (target: >95%) ‚ö†Ô∏è
  ‚Ä¢ On-time: 26 incidents
  ‚Ä¢ Late: 4 incidents

Channel Breakdown:
  ‚Ä¢ Build: 4 incidents, MTTR 4.2h
  ‚Ä¢ Lint: 6 incidents, MTTR 8.5h
  ‚Ä¢ Test: 8 incidents, MTTR 12.1h (‚ö†Ô∏è increasing trend)
  ‚Ä¢ Deployment: 2 incidents, MTTR 6.8h
  ‚Ä¢ Runtime: 4 incidents, MTTR 3.5h

Top Issues:
  1. Test flakiness in auth module (3 incidents)
  2. Lint violations accumulating in legacy code (2 incidents)
  3. Build time increasing with new dependencies (1 incident)

Recommendations:
  ‚Ä¢ Focus on fixing flaky tests in auth module
  ‚Ä¢ Schedule tech debt sprint for lint violations
  ‚Ä¢ Review recent dependency additions for build impact

Team Recognition:
  ‚Ä¢ Fastest resolution: jane@example.com (1.2h avg MTTR)
  ‚Ä¢ Most incidents resolved: john@example.com (8 resolved)

View full report: /governance/qiw/reports/weekly/2026-W02
```

---

### Monthly

**Audience**: Engineering Managers + Governance Liaison  
**Content**: Monthly trends, KPI review, governance compliance  
**Delivery**: Report generated, presentation scheduled  

**Includes**:
- Monthly KPI dashboard
- Incident trends analysis
- Channel performance comparison
- MTTR trends
- Process improvement recommendations
- Governance compliance assessment

**Sections**:
1. Executive Summary
2. Quality Metrics
3. Process Metrics
4. Channel Analysis
5. Trends and Patterns
6. Governance Compliance
7. Recommendations

---

### Quarterly

**Audience**: Executive Team + Board (if applicable)  
**Content**: Strategic review, long-term trends  
**Delivery**: Executive presentation  

**Includes**:
- Quarterly quality trends
- Year-over-year comparison
- Impact on business metrics (deployment frequency, MTBF, customer satisfaction)
- ROI of QIW implementation
- Strategic recommendations

---

## Metric Collection

### Data Sources

**Primary**:
- `governance/memory/PartPulse/qiw-events.json` (incident data)

**Secondary**:
- CI/CD logs (build, test, deployment metrics)
- Application monitoring (runtime metrics)
- Git history (code churn, PR data)
- Linter output (code quality metrics)

### Collection Frequency

- Real-time: Runtime metrics (every 1-5 minutes)
- Continuous: Build, lint, test metrics (every commit/PR)
- Hourly: Aggregated runtime metrics
- Daily: Incident summaries, MTTR calculations
- Weekly: Trend analysis, SLA compliance
- Monthly: KPI rollups, governance reports

---

## Analysis Guidelines

### Trend Analysis

**Look for**:
- Increasing incident counts (quality degrading)
- Increasing MTTR (process degrading)
- Patterns by channel (specific area needs attention)
- Patterns by team (training or resource needs)
- Correlation with deployments (new code quality)

**Red Flags**:
- Sudden spike in incidents
- MTTR trending upward
- SLA compliance dropping
- Repeat incidents increasing
- False positive rate increasing

---

### Root Cause Analysis

For patterns of incidents:
1. Group incidents by similarity
2. Identify common factors
3. Determine root cause
4. Propose systemic fix
5. Track implementation
6. Verify effectiveness

---

### Comparative Analysis

**Compare**:
- This week vs. last week
- This month vs. last month
- This quarter vs. last quarter
- Team A vs. Team B (for learning, not competition)
- Channel A vs. Channel B (resource allocation)

---

## Metric Visualization

### Charts and Graphs

**Line Charts**: Trends over time
- Quality score
- Incident count
- MTTR

**Bar Charts**: Comparisons
- Incidents by channel
- Incidents by severity
- MTTR by channel

**Pie Charts**: Distribution
- Incidents by channel (%)
- Incidents by severity (%)

**Heat Maps**: Patterns
- Incident frequency by day/time
- Incident concentration by module

**Gauges**: Current status
- Quality score
- SLA compliance
- Channel health

---

## Reporting Tools

**Recommended Tools**:
- **Dashboard**: Real-time metrics (React + Chart.js)
- **Reports**: Automated report generation (Node.js + PDFKit)
- **Analytics**: Trend analysis (Python + Pandas)
- **Alerts**: Threshold monitoring (Node.js + Nodemailer)

**Data Export**:
- CSV: For spreadsheet analysis
- JSON: For programmatic access
- PDF: For formal reports
- API: For integration with other tools

---

## Governance Reporting

### Compliance Metrics

**QIW Implementation Compliance**:
- ‚úÖ All 5 channels operational
- ‚úÖ Incident schema followed
- ‚úÖ SLA targets met
- ‚úÖ Dashboard accessible
- ‚úÖ Escalation process followed

**Audit Trail**:
- All incidents logged in `qiw-events.json`
- Append-only integrity maintained
- All escalations documented
- All bypasses approved and tracked

**Monthly Governance Report**:
```
QIW Governance Compliance Report - January 2026

Implementation Status: ‚úÖ COMPLIANT

Channel Operations:
  ‚úÖ Build channel: Operational
  ‚úÖ Lint channel: Operational
  ‚úÖ Test channel: Operational
  ‚úÖ Deployment channel: Operational
  ‚úÖ Runtime channel: Operational

Incident Management:
  ‚Ä¢ Total incidents: 87
  ‚Ä¢ Schema compliance: 100%
  ‚Ä¢ Append-only integrity: Verified ‚úì
  ‚Ä¢ SLA compliance: 87% (target: >95%) ‚ö†Ô∏è

Quality Gates:
  ‚Ä¢ Gates operational: 100%
  ‚Ä¢ Block rate: 4.2% ‚úì
  ‚Ä¢ Bypass rate: 0.8% ‚úì
  ‚Ä¢ False positive rate: 1.5% ‚úì

Process Compliance:
  ‚Ä¢ Escalation process followed: 100%
  ‚Ä¢ Incident response procedures followed: 98%
  ‚Ä¢ Documentation complete: 100%

Issues Identified:
  ‚Ä¢ SLA compliance below target (87% vs. 95%)
  ‚Ä¢ 4 incidents exceeded SLA (all High severity)

Recommendations:
  ‚Ä¢ Focus on reducing MTTR for High severity incidents
  ‚Ä¢ Consider adding resources to QA team
  ‚Ä¢ Review test channel detector accuracy

Governance Liaison Sign-Off:
  Name: governance@example.com
  Date: 2026-02-01
  Status: Approved with recommendations
```

---

## Continuous Improvement

### Metric Review Cycle

**Monthly Review**:
1. Review all metrics
2. Identify trends
3. Propose threshold adjustments
4. Implement improvements
5. Verify effectiveness

**Quarterly Review**:
1. Assess metric relevance
2. Add/remove metrics as needed
3. Update targets based on baseline
4. Strategic recommendations

---

## Best Practices

1. ‚úÖ **Track Consistently**: Don't skip metric collection
2. ‚úÖ **Analyze Regularly**: Weekly minimum for trends
3. ‚úÖ **Act on Data**: Metrics without action are wasted
4. ‚úÖ **Communicate**: Share metrics broadly
5. ‚úÖ **Improve**: Use metrics to drive improvement
6. ‚ùå **Don't Game**: Metrics should reflect reality
7. ‚ùå **Don't Overload**: Too many metrics dilute focus
8. ‚ùå **Don't Ignore**: Pay attention to concerning trends

---

**Last Updated**: 2026-01-14  
**Version**: 1.0.0  
**Status**: Active
